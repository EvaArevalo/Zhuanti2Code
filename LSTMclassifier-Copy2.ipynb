{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM classifier for abusive/sarcastic language"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import keras_metrics\n",
    "\n",
    "from pymongo import MongoClient\n",
    "from Preprocessing import config\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.wrappers.scikit_learn import KerasClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MongoDB connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection_name=\"news_sarcasm_abusive_normal\"\n",
    "#collection_name=\"sarcasm_abusive_normal\"\n",
    "#collection_name=\"abusive_normal\"\n",
    "#collection_name=\"news_abusive\"\n",
    "#collection_name=\"sarcasm_normal\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def connectToMongoDB(collection_name):\n",
    "    client = MongoClient(config.MONGODB['hostname'], config.MONGODB['port'])\n",
    "    db = client[config.MONGODB['db']]\n",
    "    collection = db[config.MONGODB[collection_name]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDatasetsFromMongoDB(collection_name):\n",
    "    ''' mongodb to pandas dataframe, export to csv and return'''\n",
    "    connectToMongoDB(collection_name)\n",
    "    results=collection.find()\n",
    "    #strip and reshuflle\n",
    "    df =  pd.DataFrame(list(results))\n",
    "    df=df[['label','text']]\n",
    "    df=df.reindex(np.random.permutation(df.index))\n",
    "    filename = collection_name +'.csv'\n",
    "    df.to_csv(filename,encoding='utf-8-sig')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDatasetsFromCsv():\n",
    "\t'''import csv, reshuffle and return it'''\n",
    "\tdf=pd.read_csv('sarcasm_and_news_dataset.csv')\n",
    "\tdf=df.reindex(np.random.permutation(df.index))\n",
    "\treturn df[['label','text']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         label                                               text\n",
      "20272   normal  No importa si no les gusta él, o Topp Dogg, o ...\n",
      "38747  sarcasm  a veces no entiendo los twees de joel si quier...\n",
      "30377  sarcasm  Métete tu sarcasmo por el centro del orto y ve...\n",
      "25977     fact  José Tomás indulta un toro en Barcelona: Barce...\n",
      "46386   normal  Poco a poco vemos como las cosas van tomando f...\n",
      "54655     fact   ÚLTIMAHORA Un ataque aéreo de EEUU mata al lí...\n",
      "27654   normal  Mi mamá es de esas personas que se come los ga...\n",
      "14992  abusive  Yo, aquí como pendejo pensando en ti, y tu bie...\n",
      "54610   normal  La esencia del Guadalajara se ha forjado graci...\n",
      "15882  sarcasm  Esos que ponen en sus bios \"sarcasmo es mi seg...\n",
      "36282   normal  Voy a hacer una saga de libros de fantasía con...\n",
      "59063   normal  No tiene una poquita de vergüenza, enserio?tan...\n",
      "63472  sarcasm  Prefiero que me manden a comer mierda y me put...\n",
      "12258   normal  Marcha por Brandon y Glen En el SPN de Tipitap...\n",
      "34083  sarcasm  Hijo de Aristóbulo Istúriz también le envió un...\n",
      "15122  abusive  Las abuelas mexicanas no tomaron ácido fólico ...\n",
      "28911  abusive  \"200 y dejo de ser fake\". –Un pendejo que creé...\n",
      "8974   abusive  2006-2011 : “PINCHE CALDERON ERES UN PENDEJO” ...\n",
      "43747  abusive  Este ... Si mira ... DE NADA SIRVE QUE ESTÉS B...\n",
      "21088  sarcasm  Daño Electrico. Bueno que en fiestas patrias y...\n",
      "7612   abusive  \"Que pinche crudota\" —Un pendejo bajo de autoe...\n",
      "14210     fact  EP Netanyahu pide nuevas elecciones en Israel ...\n",
      "5865    normal  Me dormí con la emoción de despertar y ver nue...\n",
      "74591  sarcasm  Taller \"Otras Formas de Comunicar\" / Ironia, h...\n",
      "43871   normal  Yo no tengo pelos en la lengua si te tengo que...\n",
      "28783  sarcasm  PERO ESTA EL EJERCITO DE COLOMBIA REFORZANDO L...\n",
      "40405   normal  Extraño cuando mis ojeras eran por desvelarme ...\n",
      "7332   abusive  ¡Vaya! Ya has twitteado eso... Y A TI PINCHE T...\n",
      "22856  sarcasm  \"Cheta rubia\" Ni te gastes en explicarleel sar...\n",
      "73368   normal  A Fajardo le están faltando los pocos votos de...\n",
      "...        ...                                                ...\n",
      "65968   normal  En la cadena que autocensuró la entrevista de ...\n",
      "68222     fact  EP El Gobierno levantará en breve la recomenda...\n",
      "2912    normal  A mi que Macri no se persigne me importa nada,...\n",
      "26771  abusive  \"El gobierno que México merece\" ESCUCHASTE PIN...\n",
      "19318   normal  ¿Os acordais de los que se descojonaban cuando...\n",
      "60530  sarcasm  Bastante que nos sirve tu apoyo moral , con es...\n",
      "27169  sarcasm  Claro acá no hay promocione solo simples casua...\n",
      "68453  abusive  Lo que busca  Martin Estranza es la Toma de La...\n",
      "55136  abusive  Un dia como hoy, escuchar someone like you pue...\n",
      "35259  abusive  —Profe, ¿qué es bullying? —A ver, escuchen lo ...\n",
      "5495   abusive             NI A PENDEJO LLEGAS PINCHE MORRO SARRA\n",
      "63164  sarcasm  Abraham vino a Perú 2 veces y en ambas solo vi...\n",
      "29206     fact  Ay que linda era la vida sin Macri y su manga ...\n",
      "25255  abusive  Parece que les dicen: \"Ve y enamórate de un pe...\n",
      "70171   normal  Felicidades <mention> y a todo el equipo que t...\n",
      "2468   sarcasm   sarcasmo me encantan las personas que dicen q...\n",
      "43333  abusive       JAJAJJAJAJJAJAJAJJAJA PINCHE MARRANO PENDEJO\n",
      "52665  sarcasm  <mention> No lo pierdo el episodio 71 de esta ...\n",
      "5681    normal   VotoUtilVsAmlo que no te engañen  amlover el ...\n",
      "24646  sarcasm  Jimin: —Nishiki —Frío por fuera —Sarcástico —A...\n",
      "60340  sarcasm  Un poco de  sarcasmo no esta mal entre todo el...\n",
      "10044     fact   ULTIMAHORA Mataron al periodista estadouniden...\n",
      "62730  sarcasm  Quieres decirle a neme todo lo q dices por tw?...\n",
      "28150   normal  Argentina presente en el <mention> <mention> |...\n",
      "16881  sarcasm  Si no saben jugar Twitter y se toman literal t...\n",
      "23899  sarcasm  Un antitaurino insultando? Nooooo .... si eso ...\n",
      "23067  sarcasm  El humor y el sarcasmo sigue existiendo y sigu...\n",
      "15405  abusive  Te voy a dejar un \"Me encantas\" por aquí, por ...\n",
      "67620   normal  Orgullo de este equipo. LA UNIÓN HACE LA FUERZ...\n",
      "71192   normal                Porque me cuesta soltar el teléfono\n",
      "\n",
      "[77208 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "#first time, extract from MongoDB\n",
    "#data=getDatasetsFromMongoDB('news_sarcasm_abusive_normal')\n",
    "data=getDatasetsFromCsv()\n",
    "print(data)\n",
    "train,test = train_test_split(data,test_size=0.33,random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 1000\n",
    "embedding_dim = 128\n",
    "lstm_out_dim = 196\n",
    "batch_size = 32\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0   0   0 ... 577   6  16]\n",
      " [  0   0   0 ...  47 247 580]\n",
      " [  0   0   0 ... 302 497  11]\n",
      " ...\n",
      " [  0   0   0 ...   2  17 445]\n",
      " [  0   0   0 ... 170 226  16]\n",
      " [  0   0   0 ...  49  19   3]]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(num_words=max_features, split=' ')\n",
    "tokenizer.fit_on_texts(data['text'].values)\n",
    "X = tokenizer.texts_to_sequences(data['text'].values)\n",
    "print(X)\n",
    "X = pad_sequences(X)\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get labels, split dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(51729, 63) (51729, 4)\n",
      "(12739, 63) (12739, 4)\n",
      "(12740, 63) (12740, 4)\n"
     ]
    }
   ],
   "source": [
    "Y = pd.get_dummies(data['label']).values\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size=0.33, \n",
    "    random_state=42)\n",
    "\n",
    "validation_size = math.ceil(X_test.shape[0]/2)\n",
    "\n",
    "X_validate = X_test[-validation_size:]\n",
    "Y_validate = Y_test[-validation_size:]\n",
    "X_test = X_test[:-validation_size]\n",
    "Y_test = Y_test[:-validation_size]\n",
    "\n",
    "#Get shapes\n",
    "print(X_train.shape,Y_train.shape)\n",
    "print(X_test.shape,Y_test.shape)\n",
    "print(X_validate.shape,Y_validate.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=None\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, embedding_dim,input_length=X.shape[1]))\n",
    "model.add(SpatialDropout1D(0.4))\n",
    "model.add(LSTM(lstm_out_dim, dropout=0.2,recurrent_dropout=0.2))\n",
    "model.add(Dense(4,activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam',\n",
    "              metrics=['accuracy',keras_metrics.precision(), keras_metrics.recall()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      " - 290s - loss: 0.3012 - acc: 0.8977 - precision: 0.9064 - recall: 0.8657\n",
      "Epoch 2/10\n",
      " - 282s - loss: 0.2093 - acc: 0.9295 - precision: 0.9387 - recall: 0.9217\n",
      "Epoch 3/10\n",
      " - 279s - loss: 0.1884 - acc: 0.9370 - precision: 0.9447 - recall: 0.9302\n",
      "Epoch 4/10\n",
      " - 277s - loss: 0.1776 - acc: 0.9399 - precision: 0.9474 - recall: 0.9334\n",
      "Epoch 5/10\n",
      " - 285s - loss: 0.1665 - acc: 0.9432 - precision: 0.9504 - recall: 0.9372\n",
      "Epoch 6/10\n",
      " - 285s - loss: 0.1579 - acc: 0.9454 - precision: 0.9525 - recall: 0.9396\n",
      "Epoch 7/10\n",
      " - 300s - loss: 0.1506 - acc: 0.9480 - precision: 0.9538 - recall: 0.9420\n",
      "Epoch 8/10\n",
      " - 320s - loss: 0.1427 - acc: 0.9507 - precision: 0.9568 - recall: 0.9458\n",
      "Epoch 9/10\n",
      " - 290s - loss: 0.1358 - acc: 0.9523 - precision: 0.9581 - recall: 0.9473\n",
      "Epoch 10/10\n",
      " - 303s - loss: 0.1279 - acc: 0.9544 - precision: 0.9599 - recall: 0.9501\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x23839a77940>"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, Y_train, epochs = epochs, batch_size=batch_size, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score: 0.20\n",
      "acc: 0.93\n"
     ]
    }
   ],
   "source": [
    "score,acc = model.evaluate(X_test, Y_test, verbose = 2, batch_size = batch_size)\n",
    "\n",
    "print(\"score: %.2f\" % (score))\n",
    "print(\"acc: %.2f\" % (acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 0 0]\n",
      " [0 0 1 0]\n",
      " [0 0 0 1]\n",
      " ...\n",
      " [0 0 0 1]\n",
      " [0 0 1 0]\n",
      " [0 0 1 0]]\n",
      "1 Accuracy 93.83033419023135 %\n",
      "2 Accuracy 93.06405548755609 %\n",
      "3 Accuracy 94.03935185185185 %\n",
      "4 Accuracy 94.24316514781063 %\n",
      "2190\n",
      "2334\n",
      "2281\n",
      "2451\n",
      "3250\n",
      "3456\n",
      "4240\n",
      "4499\n"
     ]
    }
   ],
   "source": [
    "print(pd.get_dummies(data['label']).values)\n",
    "l1_count = 0\n",
    "l2_count = 0\n",
    "l3_count = 0\n",
    "l4_count = 0\n",
    "l1_correct = 0\n",
    "l2_correct = 0\n",
    "l3_correct = 0\n",
    "l4_correct = 0\n",
    "\n",
    "for x in range(len(X_validate)):\n",
    "\n",
    "    result = model.predict(X_validate[x].reshape(1,X_test.shape[1]),batch_size=1,verbose = 2)[0]\n",
    "\n",
    "    if np.argmax(result) == np.argmax(Y_validate[x]):\n",
    "        if np.argmax(Y_validate[x]) == 0:\n",
    "            l1_correct += 1\n",
    "        elif np.argmax(Y_validate[x]) == 1:\n",
    "            l2_correct += 1\n",
    "        elif np.argmax(Y_validate[x]) == 2:\n",
    "            l3_correct += 1\n",
    "        else:\n",
    "            l4_correct += 1\n",
    "            \n",
    "\n",
    "    if np.argmax(Y_validate[x]) == 0:\n",
    "        l1_count += 1\n",
    "    elif np.argmax(Y_validate[x]) == 1:\n",
    "        l2_count += 1\n",
    "    elif np.argmax(Y_validate[x]) == 2:\n",
    "        l3_count += 1\n",
    "    else:\n",
    "        l4_count += 1\n",
    "\n",
    "print(\"1 Accuracy\", l1_correct/l1_count*100, \"%\")\n",
    "print(\"2 Accuracy\", l2_correct/l2_count*100, \"%\")\n",
    "print(\"3 Accuracy\", l3_correct/l3_count*100, \"%\")\n",
    "print(\"4 Accuracy\", l4_correct/l4_count*100, \"%\")\n",
    "print(l1_correct)\n",
    "print(l1_count)\n",
    "print(l2_correct)\n",
    "print(l2_count)\n",
    "print(l3_correct)\n",
    "print(l3_count)\n",
    "print(l4_correct)\n",
    "print(l4_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and get results, different embedding parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0   0   0 ...   5  21  20]\n",
      " [  0   0   0 ... 273  11  16]\n",
      " [  0   0   0 ... 741 629  16]\n",
      " ...\n",
      " [  0   0   0 ... 695  13  16]\n",
      " [  0   0   0 ...   4 969  16]\n",
      " [  0   0   0 ... 141   1  12]]\n",
      "(51729, 63) (51729, 4)\n",
      "(12739, 63) (12739, 4)\n",
      "(12740, 63) (12740, 4)\n",
      "Epoch 1/6\n",
      " - 142s - loss: 0.3041 - acc: 0.8957\n",
      "Epoch 2/6\n",
      " - 146s - loss: 0.1902 - acc: 0.9361\n",
      "Epoch 3/6\n",
      " - 131s - loss: 0.1689 - acc: 0.9429\n",
      "Epoch 4/6\n",
      " - 119s - loss: 0.1538 - acc: 0.9483\n",
      "Epoch 5/6\n",
      " - 136s - loss: 0.1409 - acc: 0.9522\n",
      "Epoch 6/6\n",
      " - 133s - loss: 0.1322 - acc: 0.9559\n",
      "score: 0.18\n",
      "acc: 0.94\n",
      "Positive Accuracy 94.33019411877763 %\n",
      "Negative Accuracy 94.17309340188518 %\n",
      "9816\n",
      "10406\n",
      "2198\n",
      "2334\n"
     ]
    }
   ],
   "source": [
    "#250 features, embedding dim 64\n",
    "max_features = 2000\n",
    "embedding_dim = 128\n",
    "lstm_out_dim = 50\n",
    "batch_size = 30\n",
    "epochs = 6\n",
    "\n",
    "train,test = train_test_split(data,test_size=0.33,random_state=42)\n",
    "tokenizer = Tokenizer(num_words=max_features, split=' ')\n",
    "tokenizer.fit_on_texts(data['text'].values)\n",
    "X = tokenizer.texts_to_sequences(data['text'].values)\n",
    "print(X)\n",
    "X = pad_sequences(X)\n",
    "print(X)\n",
    "\n",
    "Y = pd.get_dummies(data['label']).values\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size=0.33, \n",
    "    random_state=42)\n",
    "\n",
    "validation_size = math.ceil(X_test.shape[0]/2)\n",
    "\n",
    "X_validate = X_test[-validation_size:]\n",
    "Y_validate = Y_test[-validation_size:]\n",
    "X_test = X_test[:-validation_size]\n",
    "Y_test = Y_test[:-validation_size]\n",
    "\n",
    "#Get shapes\n",
    "print(X_train.shape,Y_train.shape)\n",
    "print(X_test.shape,Y_test.shape)\n",
    "print(X_validate.shape,Y_validate.shape)\n",
    "\n",
    "model=None\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, embedding_dim,input_length=X.shape[1]))\n",
    "model.add(SpatialDropout1D(0.4))\n",
    "model.add(LSTM(lstm_out_dim, dropout=0.2,recurrent_dropout=0.2))\n",
    "model.add(Dense(4,activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, Y_train, epochs = epochs, batch_size=batch_size, verbose=2)\n",
    "\n",
    "score,acc = model.evaluate(X_test, Y_test, verbose = 2, batch_size = batch_size)\n",
    "\n",
    "print(\"score: %.2f\" % (score))\n",
    "print(\"acc: %.2f\" % (acc))\n",
    "\n",
    "positive_count = 0\n",
    "negative_count = 0\n",
    "positive_correct = 0\n",
    "negative_correct = 0\n",
    "\n",
    "for x in range(len(X_validate)):\n",
    "\n",
    "    result = model.predict(X_validate[x].reshape(1,X_test.shape[1]),batch_size=1,verbose = 2)[0]\n",
    "\n",
    "    if np.argmax(result) == np.argmax(Y_validate[x]):\n",
    "        if np.argmax(Y_validate[x]) == 0:\n",
    "            negative_correct += 1\n",
    "        else:\n",
    "            positive_correct += 1\n",
    "\n",
    "    if np.argmax(Y_validate[x]) == 0:\n",
    "        negative_count += 1\n",
    "    else:\n",
    "        positive_count += 1\n",
    "\n",
    "print(\"Positive Accuracy\", positive_correct/positive_count*100, \"%\")\n",
    "print(\"Negative Accuracy\", negative_correct/negative_count*100, \"%\")\n",
    "print(positive_correct)\n",
    "print(positive_count)\n",
    "print(negative_correct)\n",
    "print(negative_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       abusive  fact  normal  sarcasm\n",
      "47531        1     0       0        0\n",
      "29525        0     0       1        0\n",
      "20997        0     0       0        1\n",
      "37791        0     0       0        1\n",
      "5511         0     0       0        1\n",
      "35350        0     0       0        1\n",
      "25814        0     0       1        0\n",
      "73592        0     0       0        1\n",
      "52798        1     0       0        0\n",
      "70399        0     0       0        1\n",
      "9777         0     0       0        1\n",
      "26321        0     0       1        0\n",
      "73848        0     0       1        0\n",
      "41459        0     1       0        0\n",
      "37179        0     0       1        0\n",
      "2654         0     0       0        1\n",
      "65348        0     0       1        0\n",
      "27881        0     0       0        1\n",
      "70677        0     1       0        0\n",
      "32769        0     0       0        1\n",
      "14535        0     0       1        0\n",
      "1774         0     0       1        0\n",
      "62431        0     0       0        1\n",
      "40329        0     0       0        1\n",
      "16008        0     1       0        0\n",
      "63165        0     0       1        0\n",
      "11532        0     0       1        0\n",
      "37980        0     1       0        0\n",
      "61855        0     0       0        1\n",
      "54225        0     0       0        1\n",
      "...        ...   ...     ...      ...\n",
      "17657        0     0       0        1\n",
      "54010        0     0       1        0\n",
      "46235        1     0       0        0\n",
      "48682        0     0       1        0\n",
      "39335        0     0       0        1\n",
      "31383        0     1       0        0\n",
      "33840        0     1       0        0\n",
      "16846        1     0       0        0\n",
      "40599        0     0       0        1\n",
      "47076        1     0       0        0\n",
      "32080        0     1       0        0\n",
      "53966        0     0       0        1\n",
      "16399        0     1       0        0\n",
      "60418        1     0       0        0\n",
      "18782        1     0       0        0\n",
      "70730        1     0       0        0\n",
      "2629         0     0       0        1\n",
      "5558         0     0       1        0\n",
      "74066        0     0       1        0\n",
      "2655         0     0       0        1\n",
      "40553        0     0       1        0\n",
      "31419        0     0       0        1\n",
      "72631        0     0       0        1\n",
      "16317        0     1       0        0\n",
      "33502        0     0       0        1\n",
      "36004        1     0       0        0\n",
      "63261        0     0       1        0\n",
      "23673        0     0       0        1\n",
      "19131        0     0       1        0\n",
      "58143        0     0       1        0\n",
      "\n",
      "[77208 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "print(pd.get_dummies(data['label']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      " - 78s - loss: 0.1174 - acc: 0.9549\n",
      "Epoch 2/10\n",
      " - 74s - loss: 0.0572 - acc: 0.9796\n",
      "Epoch 3/10\n",
      " - 72s - loss: 0.0440 - acc: 0.9840\n",
      "Epoch 4/10\n",
      " - 72s - loss: 0.0383 - acc: 0.9868\n",
      "Epoch 5/10\n",
      " - 73s - loss: 0.0320 - acc: 0.9889\n",
      "Epoch 6/10\n",
      " - 74s - loss: 0.0295 - acc: 0.9901\n",
      "Epoch 7/10\n",
      " - 66s - loss: 0.0248 - acc: 0.9910\n",
      "Epoch 8/10\n",
      " - 65s - loss: 0.0234 - acc: 0.9917\n",
      "Epoch 9/10\n",
      " - 69s - loss: 0.0184 - acc: 0.9937\n",
      "Epoch 10/10\n",
      " - 69s - loss: 0.0171 - acc: 0.9940\n",
      "score: 0.08\n",
      "acc: 0.98\n",
      "Positive Accuracy 97.82157676348547 %\n",
      "Negative Accuracy 98.17047817047818 %\n",
      "1886\n",
      "1928\n",
      "2361\n",
      "2405\n"
     ]
    }
   ],
   "source": [
    "#2000 features\n",
    "max_features = 2000\n",
    "embedding_dim = 128\n",
    "lstm_out_dim = 196\n",
    "batch_size = 32\n",
    "epochs = 10\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, embedding_dim,input_length=X.shape[1]))\n",
    "model.add(SpatialDropout1D(0.4))\n",
    "model.add(LSTM(lstm_out_dim, dropout=0.2,recurrent_dropout=0.2))\n",
    "model.add(Dense(2,activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, Y_train, epochs = epochs, batch_size=batch_size, verbose=2)\n",
    "\n",
    "score,acc = model.evaluate(X_test, Y_test, verbose = 2, batch_size = batch_size)\n",
    "\n",
    "print(\"score: %.2f\" % (score))\n",
    "print(\"acc: %.2f\" % (acc))\n",
    "\n",
    "positive_count = 0\n",
    "negative_count = 0\n",
    "positive_correct = 0\n",
    "negative_correct = 0\n",
    "\n",
    "for x in range(len(X_validate)):\n",
    "\n",
    "    result = model.predict(X_validate[x].reshape(1,X_test.shape[1]),batch_size=1,verbose = 2)[0]\n",
    "\n",
    "    if np.argmax(result) == np.argmax(Y_validate[x]):\n",
    "        if np.argmax(Y_validate[x]) == 0:\n",
    "            negative_correct += 1\n",
    "        else:\n",
    "            positive_correct += 1\n",
    "\n",
    "    if np.argmax(Y_validate[x]) == 0:\n",
    "        negative_count += 1\n",
    "    else:\n",
    "        positive_count += 1\n",
    "\n",
    "print(\"Positive Accuracy\", positive_correct/positive_count*100, \"%\")\n",
    "print(\"Negative Accuracy\", negative_correct/negative_count*100, \"%\")\n",
    "print(positive_correct)\n",
    "print(positive_count)\n",
    "print(negative_correct)\n",
    "print(negative_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      " - 79s - loss: 0.1280 - acc: 0.9508\n",
      "Epoch 2/10\n",
      " - 77s - loss: 0.0580 - acc: 0.9781\n",
      "Epoch 3/10\n",
      " - 78s - loss: 0.0440 - acc: 0.9847\n",
      "Epoch 4/10\n",
      " - 75s - loss: 0.0352 - acc: 0.9874\n",
      "Epoch 5/10\n",
      " - 72s - loss: 0.0318 - acc: 0.9884\n",
      "Epoch 6/10\n",
      " - 78s - loss: 0.0290 - acc: 0.9901\n",
      "Epoch 7/10\n",
      " - 69s - loss: 0.0255 - acc: 0.9907\n",
      "Epoch 8/10\n",
      " - 77s - loss: 0.0226 - acc: 0.9910\n",
      "Epoch 9/10\n",
      " - 77s - loss: 0.0206 - acc: 0.9931\n",
      "Epoch 10/10\n",
      " - 77s - loss: 0.0174 - acc: 0.9940\n",
      "score: 0.08\n",
      "acc: 0.98\n",
      "Positive Accuracy 98.39211618257261 %\n",
      "Negative Accuracy 97.92099792099792 %\n",
      "1897\n",
      "1928\n",
      "2355\n",
      "2405\n"
     ]
    }
   ],
   "source": [
    "#4000 features\n",
    "max_features = 4000\n",
    "embedding_dim = 128\n",
    "lstm_out_dim = 196\n",
    "batch_size = 32\n",
    "epochs = 10\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, embedding_dim,input_length=X.shape[1]))\n",
    "model.add(SpatialDropout1D(0.4))\n",
    "model.add(LSTM(lstm_out_dim, dropout=0.2,recurrent_dropout=0.2))\n",
    "model.add(Dense(2,activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, Y_train, epochs = epochs, batch_size=batch_size, verbose=2)\n",
    "\n",
    "score,acc = model.evaluate(X_test, Y_test, verbose = 2, batch_size = batch_size)\n",
    "\n",
    "print(\"score: %.2f\" % (score))\n",
    "print(\"acc: %.2f\" % (acc))\n",
    "\n",
    "positive_count = 0\n",
    "negative_count = 0\n",
    "positive_correct = 0\n",
    "negative_correct = 0\n",
    "\n",
    "for x in range(len(X_validate)):\n",
    "\n",
    "    result = model.predict(X_validate[x].reshape(1,X_test.shape[1]),batch_size=1,verbose = 2)[0]\n",
    "\n",
    "    if np.argmax(result) == np.argmax(Y_validate[x]):\n",
    "        if np.argmax(Y_validate[x]) == 0:\n",
    "            negative_correct += 1\n",
    "        else:\n",
    "            positive_correct += 1\n",
    "\n",
    "    if np.argmax(Y_validate[x]) == 0:\n",
    "        negative_count += 1\n",
    "    else:\n",
    "        positive_count += 1\n",
    "\n",
    "print(\"Positive Accuracy\", positive_correct/positive_count*100, \"%\")\n",
    "print(\"Negative Accuracy\", negative_correct/negative_count*100, \"%\")\n",
    "print(positive_correct)\n",
    "print(positive_count)\n",
    "print(negative_correct)\n",
    "print(negative_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      " - 62s - loss: 0.1251 - acc: 0.9512\n",
      "Epoch 2/10\n",
      " - 61s - loss: 0.0572 - acc: 0.9791\n",
      "Epoch 3/10\n",
      " - 62s - loss: 0.0438 - acc: 0.9849\n",
      "Epoch 4/10\n",
      " - 60s - loss: 0.0372 - acc: 0.9862\n",
      "Epoch 5/10\n",
      " - 61s - loss: 0.0307 - acc: 0.9893\n",
      "Epoch 6/10\n",
      " - 61s - loss: 0.0289 - acc: 0.9897\n",
      "Epoch 7/10\n",
      " - 63s - loss: 0.0247 - acc: 0.9908\n",
      "Epoch 8/10\n",
      " - 63s - loss: 0.0235 - acc: 0.9917\n",
      "Epoch 9/10\n",
      " - 63s - loss: 0.0195 - acc: 0.9928\n",
      "Epoch 10/10\n",
      " - 62s - loss: 0.0184 - acc: 0.9931\n",
      "score: 0.08\n",
      "acc: 0.98\n",
      "Positive Accuracy 97.82157676348547 %\n",
      "Negative Accuracy 97.83783783783784 %\n",
      "1886\n",
      "1928\n",
      "2353\n",
      "2405\n"
     ]
    }
   ],
   "source": [
    "#8000 features\n",
    "max_features = 8000\n",
    "embedding_dim = 128\n",
    "lstm_out_dim = 196\n",
    "batch_size = 32\n",
    "epochs = 10\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, embedding_dim,input_length=X.shape[1]))\n",
    "model.add(SpatialDropout1D(0.4))\n",
    "model.add(LSTM(lstm_out_dim, dropout=0.2,recurrent_dropout=0.2))\n",
    "model.add(Dense(2,activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, Y_train, epochs = epochs, batch_size=batch_size, verbose=2)\n",
    "\n",
    "score,acc = model.evaluate(X_test, Y_test, verbose = 2, batch_size = batch_size)\n",
    "\n",
    "print(\"score: %.2f\" % (score))\n",
    "print(\"acc: %.2f\" % (acc))\n",
    "\n",
    "positive_count = 0\n",
    "negative_count = 0\n",
    "positive_correct = 0\n",
    "negative_correct = 0\n",
    "\n",
    "for x in range(len(X_validate)):\n",
    "\n",
    "    result = model.predict(X_validate[x].reshape(1,X_test.shape[1]),batch_size=1,verbose = 2)[0]\n",
    "\n",
    "    if np.argmax(result) == np.argmax(Y_validate[x]):\n",
    "        if np.argmax(Y_validate[x]) == 0:\n",
    "            negative_correct += 1\n",
    "        else:\n",
    "            positive_correct += 1\n",
    "\n",
    "    if np.argmax(Y_validate[x]) == 0:\n",
    "        negative_count += 1\n",
    "    else:\n",
    "        positive_count += 1\n",
    "\n",
    "print(\"Positive Accuracy\", positive_correct/positive_count*100, \"%\")\n",
    "print(\"Negative Accuracy\", negative_correct/negative_count*100, \"%\")\n",
    "print(positive_correct)\n",
    "print(positive_count)\n",
    "print(negative_correct)\n",
    "print(negative_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#16000 features\n",
    "max_features = 16000\n",
    "embedding_dim = 128\n",
    "lstm_out_dim = 196\n",
    "batch_size = 32\n",
    "epochs = 10\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, embedding_dim,input_length=X.shape[1]))\n",
    "model.add(SpatialDropout1D(0.4))\n",
    "model.add(LSTM(lstm_out_dim, dropout=0.2,recurrent_dropout=0.2))\n",
    "model.add(Dense(2,activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, Y_train, epochs = epochs, batch_size=batch_size, verbose=2)\n",
    "\n",
    "score,acc = model.evaluate(X_test, Y_test, verbose = 2, batch_size = batch_size)\n",
    "\n",
    "print(\"score: %.2f\" % (score))\n",
    "print(\"acc: %.2f\" % (acc))\n",
    "\n",
    "positive_count = 0\n",
    "negative_count = 0\n",
    "positive_correct = 0\n",
    "negative_correct = 0\n",
    "\n",
    "for x in range(len(X_validate)):\n",
    "\n",
    "    result = model.predict(X_validate[x].reshape(1,X_test.shape[1]),batch_size=1,verbose = 2)[0]\n",
    "\n",
    "    if np.argmax(result) == np.argmax(Y_validate[x]):\n",
    "        if np.argmax(Y_validate[x]) == 0:\n",
    "            negative_correct += 1\n",
    "        else:\n",
    "            positive_correct += 1\n",
    "\n",
    "    if np.argmax(Y_validate[x]) == 0:\n",
    "        negative_count += 1\n",
    "    else:\n",
    "        positive_count += 1\n",
    "\n",
    "print(\"Positive Accuracy\", positive_correct/positive_count*100, \"%\")\n",
    "print(\"Negative Accuracy\", negative_correct/negative_count*100, \"%\")\n",
    "print(positive_correct)\n",
    "print(positive_count)\n",
    "print(negative_correct)\n",
    "print(negative_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      " - 88s - loss: 0.1100 - acc: 0.9608\n",
      "Epoch 2/15\n",
      " - 85s - loss: 0.0511 - acc: 0.9809\n",
      "Epoch 3/15\n",
      " - 84s - loss: 0.0421 - acc: 0.9857\n",
      "Epoch 4/15\n",
      " - 84s - loss: 0.0337 - acc: 0.9883\n",
      "Epoch 5/15\n",
      " - 81s - loss: 0.0294 - acc: 0.9898\n",
      "Epoch 6/15\n",
      " - 86s - loss: 0.0252 - acc: 0.9908\n",
      "Epoch 7/15\n",
      " - 101s - loss: 0.0218 - acc: 0.9932\n",
      "Epoch 8/15\n",
      " - 97s - loss: 0.0188 - acc: 0.9930\n",
      "Epoch 9/15\n",
      " - 103s - loss: 0.0168 - acc: 0.9944\n",
      "Epoch 10/15\n",
      " - 103s - loss: 0.0137 - acc: 0.9955\n",
      "Epoch 11/15\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-faa0a61b67bb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     14\u001b[0m               metrics=['accuracy'])\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[0mscore\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0macc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\evaarevalo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\models.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1000\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1001\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1002\u001b[1;33m                               validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1003\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1004\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32mc:\\users\\evaarevalo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1703\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1704\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1705\u001b[1;33m                               validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1706\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1707\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32mc:\\users\\evaarevalo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[1;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m   1234\u001b[0m                         \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1235\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1236\u001b[1;33m                     \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1237\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1238\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\evaarevalo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2480\u001b[0m         \u001b[0msession\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2481\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[1;32m-> 2482\u001b[1;33m                               **self.session_kwargs)\n\u001b[0m\u001b[0;32m   2483\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2484\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\evaarevalo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    898\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 900\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    901\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\evaarevalo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1133\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1134\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1135\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1136\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1137\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\evaarevalo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1314\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1315\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1316\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1317\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1318\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\evaarevalo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1320\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1321\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1322\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1323\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1324\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\evaarevalo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1305\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1306\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1307\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1308\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1309\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\evaarevalo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1407\u001b[0m       return tf_session.TF_SessionRun_wrapper(\n\u001b[0;32m   1408\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1409\u001b[1;33m           run_metadata)\n\u001b[0m\u001b[0;32m   1410\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1411\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#2000 features, embedding dim 256\n",
    "max_features = 2000\n",
    "embedding_dim = 256\n",
    "lstm_out_dim = 196\n",
    "batch_size = 32\n",
    "epochs = 15\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, embedding_dim,input_length=X.shape[1]))\n",
    "model.add(SpatialDropout1D(0.4))\n",
    "model.add(LSTM(lstm_out_dim, dropout=0.2,recurrent_dropout=0.2))\n",
    "model.add(Dense(2,activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, Y_train, epochs = epochs, batch_size=batch_size, verbose=2)\n",
    "\n",
    "score,acc = model.evaluate(X_test, Y_test, verbose = 2, batch_size = batch_size)\n",
    "\n",
    "print(\"score: %.2f\" % (score))\n",
    "print(\"acc: %.2f\" % (acc))\n",
    "\n",
    "positive_count = 0\n",
    "negative_count = 0\n",
    "positive_correct = 0\n",
    "negative_correct = 0\n",
    "\n",
    "for x in range(len(X_validate)):\n",
    "\n",
    "    result = model.predict(X_validate[x].reshape(1,X_test.shape[1]),batch_size=1,verbose = 2)[0]\n",
    "\n",
    "    if np.argmax(result) == np.argmax(Y_validate[x]):\n",
    "        if np.argmax(Y_validate[x]) == 0:\n",
    "            negative_correct += 1\n",
    "        else:\n",
    "            positive_correct += 1\n",
    "\n",
    "    if np.argmax(Y_validate[x]) == 0:\n",
    "        negative_count += 1\n",
    "    else:\n",
    "        positive_count += 1\n",
    "\n",
    "print(\"Positive Accuracy\", positive_correct/positive_count*100, \"%\")\n",
    "print(\"Negative Accuracy\", negative_correct/negative_count*100, \"%\")\n",
    "print(positive_correct)\n",
    "print(positive_count)\n",
    "print(negative_correct)\n",
    "print(negative_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score: 0.09\n",
      "acc: 0.98\n",
      "Positive Accuracy 96.99170124481327 %\n",
      "Negative Accuracy 98.46153846153847 %\n",
      "1870\n",
      "1928\n",
      "2368\n",
      "2405\n"
     ]
    }
   ],
   "source": [
    "score,acc = model.evaluate(X_test, Y_test, verbose = 2, batch_size = batch_size)\n",
    "\n",
    "print(\"score: %.2f\" % (score))\n",
    "print(\"acc: %.2f\" % (acc))\n",
    "\n",
    "positive_count = 0\n",
    "negative_count = 0\n",
    "positive_correct = 0\n",
    "negative_correct = 0\n",
    "\n",
    "for x in range(len(X_validate)):\n",
    "\n",
    "    result = model.predict(X_validate[x].reshape(1,X_test.shape[1]),batch_size=1,verbose = 2)[0]\n",
    "\n",
    "    if np.argmax(result) == np.argmax(Y_validate[x]):\n",
    "        if np.argmax(Y_validate[x]) == 0:\n",
    "            negative_correct += 1\n",
    "        else:\n",
    "            positive_correct += 1\n",
    "\n",
    "    if np.argmax(Y_validate[x]) == 0:\n",
    "        negative_count += 1\n",
    "    else:\n",
    "        positive_count += 1\n",
    "\n",
    "print(\"Positive Accuracy\", positive_correct/positive_count*100, \"%\")\n",
    "print(\"Negative Accuracy\", negative_correct/negative_count*100, \"%\")\n",
    "print(positive_correct)\n",
    "print(positive_count)\n",
    "print(negative_correct)\n",
    "print(negative_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      " - 59s - loss: 0.1355 - acc: 0.9440\n",
      "Epoch 2/10\n",
      " - 61s - loss: 0.0646 - acc: 0.9760\n",
      "Epoch 3/10\n",
      " - 60s - loss: 0.0489 - acc: 0.9827\n",
      "Epoch 4/10\n",
      " - 59s - loss: 0.0405 - acc: 0.9852\n",
      "Epoch 5/10\n",
      " - 63s - loss: 0.0375 - acc: 0.9867\n",
      "Epoch 6/10\n",
      " - 60s - loss: 0.0353 - acc: 0.9875\n",
      "Epoch 7/10\n",
      " - 62s - loss: 0.0293 - acc: 0.9897\n",
      "Epoch 8/10\n",
      " - 56s - loss: 0.0278 - acc: 0.9906\n",
      "Epoch 9/10\n",
      " - 60s - loss: 0.0250 - acc: 0.9915\n",
      "Epoch 10/10\n",
      " - 57s - loss: 0.0227 - acc: 0.9922\n",
      "score: 0.09\n",
      "acc: 0.97\n",
      "Positive Accuracy 95.33195020746888 %\n",
      "Negative Accuracy 98.54469854469855 %\n",
      "1838\n",
      "1928\n",
      "2370\n",
      "2405\n"
     ]
    }
   ],
   "source": [
    "#4000 features, embedding dim 64\n",
    "max_features = 4000\n",
    "embedding_dim = 64\n",
    "lstm_out_dim = 196\n",
    "batch_size = 32\n",
    "epochs = 10\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, embedding_dim,input_length=X.shape[1]))\n",
    "model.add(SpatialDropout1D(0.4))\n",
    "model.add(LSTM(lstm_out_dim, dropout=0.2,recurrent_dropout=0.2))\n",
    "model.add(Dense(2,activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, Y_train, epochs = epochs, batch_size=batch_size, verbose=2)\n",
    "\n",
    "score,acc = model.evaluate(X_test, Y_test, verbose = 2, batch_size = batch_size)\n",
    "\n",
    "print(\"score: %.2f\" % (score))\n",
    "print(\"acc: %.2f\" % (acc))\n",
    "\n",
    "positive_count = 0\n",
    "negative_count = 0\n",
    "positive_correct = 0\n",
    "negative_correct = 0\n",
    "\n",
    "for x in range(len(X_validate)):\n",
    "\n",
    "    result = model.predict(X_validate[x].reshape(1,X_test.shape[1]),batch_size=1,verbose = 2)[0]\n",
    "\n",
    "    if np.argmax(result) == np.argmax(Y_validate[x]):\n",
    "        if np.argmax(Y_validate[x]) == 0:\n",
    "            negative_correct += 1\n",
    "        else:\n",
    "            positive_correct += 1\n",
    "\n",
    "    if np.argmax(Y_validate[x]) == 0:\n",
    "        negative_count += 1\n",
    "    else:\n",
    "        positive_count += 1\n",
    "\n",
    "print(\"Positive Accuracy\", positive_correct/positive_count*100, \"%\")\n",
    "print(\"Negative Accuracy\", negative_correct/negative_count*100, \"%\")\n",
    "print(positive_correct)\n",
    "print(positive_count)\n",
    "print(negative_correct)\n",
    "print(negative_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10 fold Cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Fold 1 /10\n",
      "Epoch 1/10\n",
      " - 103s - loss: 0.1408 - acc: 0.9432\n",
      "Epoch 2/10\n",
      " - 97s - loss: 0.0616 - acc: 0.9773\n",
      "Epoch 3/10\n",
      " - 97s - loss: 0.0478 - acc: 0.9835\n",
      "Epoch 4/10\n",
      " - 97s - loss: 0.0419 - acc: 0.9847\n",
      "Epoch 5/10\n",
      " - 97s - loss: 0.0366 - acc: 0.9869\n",
      "Epoch 6/10\n",
      " - 87s - loss: 0.0318 - acc: 0.9891\n",
      "Epoch 7/10\n",
      " - 86s - loss: 0.0314 - acc: 0.9893\n",
      "Epoch 8/10\n",
      " - 89s - loss: 0.0272 - acc: 0.9907\n",
      "Epoch 9/10\n",
      " - 94s - loss: 0.0249 - acc: 0.9914\n",
      "Epoch 10/10\n",
      " - 99s - loss: 0.0232 - acc: 0.9919\n",
      "score: 0.08\n",
      "acc: 0.98\n",
      "Running Fold 2 /10\n",
      "Epoch 1/10\n",
      " - 111s - loss: 0.1367 - acc: 0.9449\n",
      "Epoch 2/10\n",
      " - 105s - loss: 0.0601 - acc: 0.9780\n",
      "Epoch 3/10\n",
      " - 105s - loss: 0.0487 - acc: 0.9822\n",
      "Epoch 4/10\n",
      " - 104s - loss: 0.0398 - acc: 0.9868\n",
      "Epoch 5/10\n",
      " - 104s - loss: 0.0361 - acc: 0.9875\n",
      "Epoch 6/10\n",
      " - 105s - loss: 0.0326 - acc: 0.9887\n",
      "Epoch 7/10\n",
      " - 105s - loss: 0.0290 - acc: 0.9889\n",
      "Epoch 8/10\n",
      " - 106s - loss: 0.0281 - acc: 0.9891\n",
      "Epoch 9/10\n",
      " - 107s - loss: 0.0247 - acc: 0.9914\n",
      "Epoch 10/10\n",
      " - 111s - loss: 0.0225 - acc: 0.9924\n",
      "score: 0.07\n",
      "acc: 0.98\n",
      "Running Fold 3 /10\n",
      "Epoch 1/10\n",
      " - 133s - loss: 0.1343 - acc: 0.9467\n",
      "Epoch 2/10\n",
      " - 123s - loss: 0.0616 - acc: 0.9781\n",
      "Epoch 3/10\n",
      " - 113s - loss: 0.0475 - acc: 0.9829\n",
      "Epoch 4/10\n",
      " - 115s - loss: 0.0387 - acc: 0.9861\n",
      "Epoch 5/10\n",
      " - 115s - loss: 0.0348 - acc: 0.9876\n",
      "Epoch 6/10\n",
      " - 109s - loss: 0.0318 - acc: 0.9893\n",
      "Epoch 7/10\n",
      " - 113s - loss: 0.0299 - acc: 0.9887\n",
      "Epoch 8/10\n",
      " - 100s - loss: 0.0255 - acc: 0.9915\n",
      "Epoch 9/10\n",
      " - 114s - loss: 0.0246 - acc: 0.9910\n",
      "Epoch 10/10\n",
      " - 109s - loss: 0.0225 - acc: 0.9922\n",
      "score: 0.07\n",
      "acc: 0.98\n",
      "Running Fold 4 /10\n",
      "Epoch 1/10\n",
      " - 130s - loss: 0.1349 - acc: 0.9447\n",
      "Epoch 2/10\n",
      " - 113s - loss: 0.0608 - acc: 0.9785\n",
      "Epoch 3/10\n",
      " - 117s - loss: 0.0493 - acc: 0.9819\n",
      "Epoch 4/10\n",
      " - 114s - loss: 0.0414 - acc: 0.9860\n",
      "Epoch 5/10\n",
      " - 123s - loss: 0.0383 - acc: 0.9876\n",
      "Epoch 6/10\n",
      " - 116s - loss: 0.0334 - acc: 0.9881\n",
      "Epoch 7/10\n",
      " - 118s - loss: 0.0317 - acc: 0.9882\n",
      "Epoch 8/10\n",
      " - 117s - loss: 0.0278 - acc: 0.9905\n",
      "Epoch 9/10\n",
      " - 117s - loss: 0.0258 - acc: 0.9910\n",
      "Epoch 10/10\n",
      " - 128s - loss: 0.0229 - acc: 0.9925\n",
      "score: 0.08\n",
      "acc: 0.98\n",
      "Running Fold 5 /10\n",
      "Epoch 1/10\n",
      " - 132s - loss: 0.1385 - acc: 0.9422\n",
      "Epoch 2/10\n",
      " - 124s - loss: 0.0603 - acc: 0.9785\n",
      "Epoch 3/10\n",
      " - 132s - loss: 0.0471 - acc: 0.9824\n",
      "Epoch 4/10\n",
      " - 135s - loss: 0.0420 - acc: 0.9843\n",
      "Epoch 5/10\n",
      " - 134s - loss: 0.0383 - acc: 0.9867\n",
      "Epoch 6/10\n",
      " - 137s - loss: 0.0315 - acc: 0.9895\n",
      "Epoch 7/10\n",
      " - 122s - loss: 0.0298 - acc: 0.9891\n",
      "Epoch 8/10\n",
      " - 118s - loss: 0.0261 - acc: 0.9915\n",
      "Epoch 9/10\n",
      " - 118s - loss: 0.0241 - acc: 0.9914\n",
      "Epoch 10/10\n",
      " - 122s - loss: 0.0238 - acc: 0.9913\n",
      "score: 0.07\n",
      "acc: 0.98\n",
      "Running Fold 6 /10\n",
      "Epoch 1/10\n",
      " - 143s - loss: 0.1376 - acc: 0.9515\n",
      "Epoch 2/10\n",
      " - 140s - loss: 0.0627 - acc: 0.9775\n",
      "Epoch 3/10\n",
      " - 130s - loss: 0.0496 - acc: 0.9815\n",
      "Epoch 4/10\n",
      " - 138s - loss: 0.0406 - acc: 0.9854\n",
      "Epoch 5/10\n",
      " - 136s - loss: 0.0373 - acc: 0.9869\n",
      "Epoch 6/10\n",
      " - 131s - loss: 0.0344 - acc: 0.9877\n",
      "Epoch 7/10\n",
      " - 130s - loss: 0.0293 - acc: 0.9898\n",
      "Epoch 8/10\n",
      " - 137s - loss: 0.0287 - acc: 0.9891\n",
      "Epoch 9/10\n",
      " - 126s - loss: 0.0257 - acc: 0.9906\n",
      "Epoch 10/10\n",
      " - 131s - loss: 0.0227 - acc: 0.9919\n",
      "score: 0.07\n",
      "acc: 0.98\n",
      "Running Fold 7 /10\n",
      "Epoch 1/10\n",
      " - 165s - loss: 0.1376 - acc: 0.9443\n",
      "Epoch 2/10\n",
      " - 148s - loss: 0.0633 - acc: 0.9775\n",
      "Epoch 3/10\n",
      " - 155s - loss: 0.0488 - acc: 0.9820\n",
      "Epoch 4/10\n",
      " - 151s - loss: 0.0410 - acc: 0.9847\n",
      "Epoch 5/10\n",
      " - 166s - loss: 0.0349 - acc: 0.9872\n",
      "Epoch 6/10\n",
      " - 154s - loss: 0.0321 - acc: 0.9887\n",
      "Epoch 7/10\n",
      " - 151s - loss: 0.0289 - acc: 0.9898\n",
      "Epoch 8/10\n",
      " - 147s - loss: 0.0267 - acc: 0.9910\n",
      "Epoch 9/10\n",
      " - 149s - loss: 0.0265 - acc: 0.9914\n",
      "Epoch 10/10\n",
      " - 154s - loss: 0.0235 - acc: 0.9916\n",
      "score: 0.06\n",
      "acc: 0.98\n",
      "Running Fold 8 /10\n",
      "Epoch 1/10\n",
      " - 154s - loss: 0.1368 - acc: 0.9451\n",
      "Epoch 2/10\n",
      " - 153s - loss: 0.0596 - acc: 0.9787\n",
      "Epoch 3/10\n",
      " - 146s - loss: 0.0520 - acc: 0.9812\n",
      "Epoch 4/10\n",
      " - 145s - loss: 0.0443 - acc: 0.9841\n",
      "Epoch 5/10\n",
      " - 151s - loss: 0.0362 - acc: 0.9869\n",
      "Epoch 6/10\n",
      " - 166s - loss: 0.0335 - acc: 0.9882\n",
      "Epoch 7/10\n",
      " - 158s - loss: 0.0298 - acc: 0.9890\n",
      "Epoch 8/10\n",
      " - 168s - loss: 0.0282 - acc: 0.9897\n",
      "Epoch 9/10\n",
      " - 165s - loss: 0.0268 - acc: 0.9906\n",
      "Epoch 10/10\n",
      " - 163s - loss: 0.0234 - acc: 0.9918\n",
      "score: 0.08\n",
      "acc: 0.98\n",
      "Running Fold 9 /10\n",
      "Epoch 1/10\n",
      " - 167s - loss: 0.1353 - acc: 0.9442\n",
      "Epoch 2/10\n",
      " - 167s - loss: 0.0628 - acc: 0.9778\n",
      "Epoch 3/10\n",
      " - 169s - loss: 0.0483 - acc: 0.9822\n",
      "Epoch 4/10\n",
      " - 195s - loss: 0.0416 - acc: 0.9853\n",
      "Epoch 5/10\n",
      " - 183s - loss: 0.0391 - acc: 0.9870\n",
      "Epoch 6/10\n",
      " - 163s - loss: 0.0323 - acc: 0.9885\n",
      "Epoch 7/10\n",
      " - 160s - loss: 0.0321 - acc: 0.9892\n",
      "Epoch 8/10\n",
      " - 161s - loss: 0.0291 - acc: 0.9892\n",
      "Epoch 9/10\n",
      " - 162s - loss: 0.0247 - acc: 0.9911\n",
      "Epoch 10/10\n",
      " - 164s - loss: 0.0241 - acc: 0.9921\n",
      "score: 0.07\n",
      "acc: 0.98\n",
      "Running Fold 10 /10\n",
      "Epoch 1/10\n",
      " - 177s - loss: 0.1364 - acc: 0.9459\n",
      "Epoch 2/10\n",
      " - 172s - loss: 0.0611 - acc: 0.9790\n",
      "Epoch 3/10\n",
      " - 192s - loss: 0.0491 - acc: 0.9831\n",
      "Epoch 4/10\n",
      " - 176s - loss: 0.0415 - acc: 0.9861\n",
      "Epoch 5/10\n",
      " - 169s - loss: 0.0369 - acc: 0.9864\n",
      "Epoch 6/10\n",
      " - 174s - loss: 0.0328 - acc: 0.9890\n",
      "Epoch 7/10\n",
      " - 172s - loss: 0.0291 - acc: 0.9904\n",
      "Epoch 8/10\n",
      " - 170s - loss: 0.0295 - acc: 0.9892\n",
      "Epoch 9/10\n",
      " - 168s - loss: 0.0254 - acc: 0.9904\n",
      "Epoch 10/10\n",
      " - 172s - loss: 0.0239 - acc: 0.9919\n",
      "score: 0.07\n",
      "acc: 0.98\n",
      "sklearn.cross_validation.StratifiedKFold(labels=[0 1 0 ... 1 0 0], n_folds=10, shuffle=True, random_state=None)\n"
     ]
    }
   ],
   "source": [
    "max_features = 4000\n",
    "embedding_dim = 64\n",
    "lstm_out_dim = 196\n",
    "batch_size = 32\n",
    "epochs = 10\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, embedding_dim,input_length=X.shape[1]))\n",
    "model.add(SpatialDropout1D(0.4))\n",
    "model.add(LSTM(lstm_out_dim, dropout=0.2,recurrent_dropout=0.2))\n",
    "model.add(Dense(2,activa tion='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "n_folds = 10\n",
    "#print(Y.shape)\n",
    "#print(Y)\n",
    "Y_reshaped=[]\n",
    "for item in Y:\n",
    "    Y_reshaped.append(item[0])\n",
    "    #print(item[0])\n",
    "skf = StratifiedKFold(Y_reshaped, n_folds=n_folds, shuffle=True)\n",
    "\n",
    "\n",
    "for i, (train, test) in enumerate(skf):\n",
    "        print (\"Running Fold\", i+1, \"/10\",)\n",
    "        model=None\n",
    "        model = Sequential()\n",
    "        model.add(Embedding(max_features, embedding_dim,input_length=X.shape[1]))\n",
    "        model.add(SpatialDropout1D(0.4))\n",
    "        model.add(LSTM(lstm_out_dim, dropout=0.2,recurrent_dropout=0.2))\n",
    "        model.add(Dense(2,activation='softmax'))\n",
    "        model.compile(loss='categorical_crossentropy', optimizer='adam',\n",
    "                      metrics=['accuracy'])\n",
    "        print(model)\n",
    "        model.fit(X_train, Y_train, epochs = epochs, batch_size=batch_size, verbose=2)\n",
    "        score,acc = model.evaluate(X_test, Y_test, verbose = 2, batch_size = batch_size)\n",
    "        print(\"score: %.2f\" % (score))\n",
    "        print(\"acc: %.2f\" % (acc))\n",
    "print(skf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Running Fold 1 /10\n",
    "Epoch 1/10\n",
    " - 103s - loss: 0.1408 - acc: 0.9432\n",
    "Epoch 2/10\n",
    " - 97s - loss: 0.0616 - acc: 0.9773\n",
    "Epoch 3/10\n",
    " - 97s - loss: 0.0478 - acc: 0.9835\n",
    "Epoch 4/10\n",
    " - 97s - loss: 0.0419 - acc: 0.9847\n",
    "Epoch 5/10\n",
    " - 97s - loss: 0.0366 - acc: 0.9869\n",
    "Epoch 6/10\n",
    " - 87s - loss: 0.0318 - acc: 0.9891\n",
    "Epoch 7/10\n",
    " - 86s - loss: 0.0314 - acc: 0.9893\n",
    "Epoch 8/10\n",
    " - 89s - loss: 0.0272 - acc: 0.9907\n",
    "Epoch 9/10\n",
    " - 94s - loss: 0.0249 - acc: 0.9914\n",
    "Epoch 10/10\n",
    " - 99s - loss: 0.0232 - acc: 0.9919\n",
    "score: 0.08\n",
    "acc: 0.98\n",
    "Running Fold 2 /10\n",
    "Epoch 1/10\n",
    " - 111s - loss: 0.1367 - acc: 0.9449\n",
    "Epoch 2/10\n",
    " - 105s - loss: 0.0601 - acc: 0.9780\n",
    "Epoch 3/10\n",
    " - 105s - loss: 0.0487 - acc: 0.9822\n",
    "Epoch 4/10\n",
    " - 104s - loss: 0.0398 - acc: 0.9868\n",
    "Epoch 5/10\n",
    " - 104s - loss: 0.0361 - acc: 0.9875\n",
    "Epoch 6/10\n",
    " - 105s - loss: 0.0326 - acc: 0.9887\n",
    "Epoch 7/10\n",
    " - 105s - loss: 0.0290 - acc: 0.9889\n",
    "Epoch 8/10\n",
    " - 106s - loss: 0.0281 - acc: 0.9891\n",
    "Epoch 9/10\n",
    " - 107s - loss: 0.0247 - acc: 0.9914\n",
    "Epoch 10/10\n",
    " - 111s - loss: 0.0225 - acc: 0.9924\n",
    "score: 0.07\n",
    "acc: 0.98\n",
    "Running Fold 3 /10\n",
    "Epoch 1/10\n",
    " - 133s - loss: 0.1343 - acc: 0.9467\n",
    "Epoch 2/10\n",
    " - 123s - loss: 0.0616 - acc: 0.9781\n",
    "Epoch 3/10\n",
    " - 113s - loss: 0.0475 - acc: 0.9829\n",
    "Epoch 4/10\n",
    " - 115s - loss: 0.0387 - acc: 0.9861\n",
    "Epoch 5/10\n",
    " - 115s - loss: 0.0348 - acc: 0.9876\n",
    "Epoch 6/10\n",
    " - 109s - loss: 0.0318 - acc: 0.9893\n",
    "Epoch 7/10\n",
    " - 113s - loss: 0.0299 - acc: 0.9887\n",
    "Epoch 8/10\n",
    " - 100s - loss: 0.0255 - acc: 0.9915\n",
    "Epoch 9/10\n",
    " - 114s - loss: 0.0246 - acc: 0.9910\n",
    "Epoch 10/10\n",
    " - 109s - loss: 0.0225 - acc: 0.9922\n",
    "score: 0.07\n",
    "acc: 0.98\n",
    "Running Fold 4 /10\n",
    "Epoch 1/10\n",
    " - 130s - loss: 0.1349 - acc: 0.9447\n",
    "Epoch 2/10\n",
    " - 113s - loss: 0.0608 - acc: 0.9785\n",
    "Epoch 3/10\n",
    " - 117s - loss: 0.0493 - acc: 0.9819\n",
    "Epoch 4/10\n",
    " - 114s - loss: 0.0414 - acc: 0.9860\n",
    "Epoch 5/10\n",
    " - 123s - loss: 0.0383 - acc: 0.9876\n",
    "Epoch 6/10\n",
    " - 116s - loss: 0.0334 - acc: 0.9881\n",
    "Epoch 7/10\n",
    " - 118s - loss: 0.0317 - acc: 0.9882\n",
    "Epoch 8/10\n",
    " - 117s - loss: 0.0278 - acc: 0.9905\n",
    "Epoch 9/10\n",
    " - 117s - loss: 0.0258 - acc: 0.9910\n",
    "Epoch 10/10\n",
    " - 128s - loss: 0.0229 - acc: 0.9925\n",
    "score: 0.08\n",
    "acc: 0.98\n",
    "Running Fold 5 /10\n",
    "Epoch 1/10\n",
    " - 132s - loss: 0.1385 - acc: 0.9422\n",
    "Epoch 2/10\n",
    " - 124s - loss: 0.0603 - acc: 0.9785\n",
    "Epoch 3/10\n",
    " - 132s - loss: 0.0471 - acc: 0.9824\n",
    "Epoch 4/10\n",
    " - 135s - loss: 0.0420 - acc: 0.9843\n",
    "Epoch 5/10\n",
    " - 134s - loss: 0.0383 - acc: 0.9867\n",
    "Epoch 6/10\n",
    " - 137s - loss: 0.0315 - acc: 0.9895\n",
    "Epoch 7/10\n",
    " - 122s - loss: 0.0298 - acc: 0.9891\n",
    "Epoch 8/10\n",
    " - 118s - loss: 0.0261 - acc: 0.9915\n",
    "Epoch 9/10\n",
    " - 118s - loss: 0.0241 - acc: 0.9914\n",
    "Epoch 10/10\n",
    " - 122s - loss: 0.0238 - acc: 0.9913\n",
    "score: 0.07\n",
    "acc: 0.98\n",
    "Running Fold 6 /10\n",
    "Epoch 1/10\n",
    " - 143s - loss: 0.1376 - acc: 0.9515\n",
    "Epoch 2/10\n",
    " - 140s - loss: 0.0627 - acc: 0.9775\n",
    "Epoch 3/10\n",
    " - 130s - loss: 0.0496 - acc: 0.9815\n",
    "Epoch 4/10\n",
    " - 138s - loss: 0.0406 - acc: 0.9854\n",
    "Epoch 5/10\n",
    " - 136s - loss: 0.0373 - acc: 0.9869\n",
    "Epoch 6/10\n",
    " - 131s - loss: 0.0344 - acc: 0.9877\n",
    "Epoch 7/10\n",
    " - 130s - loss: 0.0293 - acc: 0.9898\n",
    "Epoch 8/10\n",
    " - 137s - loss: 0.0287 - acc: 0.9891\n",
    "Epoch 9/10\n",
    " - 126s - loss: 0.0257 - acc: 0.9906\n",
    "Epoch 10/10\n",
    " - 131s - loss: 0.0227 - acc: 0.9919\n",
    "score: 0.07\n",
    "acc: 0.98\n",
    "Running Fold 7 /10\n",
    "Epoch 1/10\n",
    " - 165s - loss: 0.1376 - acc: 0.9443\n",
    "Epoch 2/10\n",
    " - 148s - loss: 0.0633 - acc: 0.9775\n",
    "Epoch 3/10\n",
    " - 155s - loss: 0.0488 - acc: 0.9820\n",
    "Epoch 4/10\n",
    " - 151s - loss: 0.0410 - acc: 0.9847\n",
    "Epoch 5/10\n",
    " - 166s - loss: 0.0349 - acc: 0.9872\n",
    "Epoch 6/10\n",
    " - 154s - loss: 0.0321 - acc: 0.9887\n",
    "Epoch 7/10\n",
    " - 151s - loss: 0.0289 - acc: 0.9898\n",
    "Epoch 8/10\n",
    " - 147s - loss: 0.0267 - acc: 0.9910\n",
    "Epoch 9/10\n",
    " - 149s - loss: 0.0265 - acc: 0.9914\n",
    "Epoch 10/10\n",
    " - 154s - loss: 0.0235 - acc: 0.9916\n",
    "score: 0.06\n",
    "acc: 0.98\n",
    "Running Fold 8 /10\n",
    "Epoch 1/10\n",
    " - 154s - loss: 0.1368 - acc: 0.9451\n",
    "Epoch 2/10\n",
    " - 153s - loss: 0.0596 - acc: 0.9787\n",
    "Epoch 3/10\n",
    " - 146s - loss: 0.0520 - acc: 0.9812\n",
    "Epoch 4/10\n",
    " - 145s - loss: 0.0443 - acc: 0.9841\n",
    "Epoch 5/10\n",
    " - 151s - loss: 0.0362 - acc: 0.9869\n",
    "Epoch 6/10\n",
    " - 166s - loss: 0.0335 - acc: 0.9882\n",
    "Epoch 7/10\n",
    " - 158s - loss: 0.0298 - acc: 0.9890\n",
    "Epoch 8/10\n",
    " - 168s - loss: 0.0282 - acc: 0.9897\n",
    "Epoch 9/10\n",
    " - 165s - loss: 0.0268 - acc: 0.9906\n",
    "Epoch 10/10\n",
    " - 163s - loss: 0.0234 - acc: 0.9918\n",
    "score: 0.08\n",
    "acc: 0.98\n",
    "Running Fold 9 /10\n",
    "Epoch 1/10\n",
    " - 167s - loss: 0.1353 - acc: 0.9442\n",
    "Epoch 2/10\n",
    " - 167s - loss: 0.0628 - acc: 0.9778\n",
    "Epoch 3/10\n",
    " - 169s - loss: 0.0483 - acc: 0.9822\n",
    "Epoch 4/10\n",
    " - 195s - loss: 0.0416 - acc: 0.9853\n",
    "Epoch 5/10\n",
    " - 183s - loss: 0.0391 - acc: 0.9870\n",
    "Epoch 6/10\n",
    " - 163s - loss: 0.0323 - acc: 0.9885\n",
    "Epoch 7/10\n",
    " - 160s - loss: 0.0321 - acc: 0.9892\n",
    "Epoch 8/10\n",
    " - 161s - loss: 0.0291 - acc: 0.9892\n",
    "Epoch 9/10\n",
    " - 162s - loss: 0.0247 - acc: 0.9911\n",
    "Epoch 10/10\n",
    " - 164s - loss: 0.0241 - acc: 0.9921\n",
    "score: 0.07\n",
    "acc: 0.98\n",
    "Running Fold 10 /10\n",
    "Epoch 1/10\n",
    " - 177s - loss: 0.1364 - acc: 0.9459\n",
    "Epoch 2/10\n",
    " - 172s - loss: 0.0611 - acc: 0.9790\n",
    "Epoch 3/10\n",
    " - 192s - loss: 0.0491 - acc: 0.9831\n",
    "Epoch 4/10\n",
    " - 176s - loss: 0.0415 - acc: 0.9861\n",
    "Epoch 5/10\n",
    " - 169s - loss: 0.0369 - acc: 0.9864\n",
    "Epoch 6/10\n",
    " - 174s - loss: 0.0328 - acc: 0.9890\n",
    "Epoch 7/10\n",
    " - 172s - loss: 0.0291 - acc: 0.9904\n",
    "Epoch 8/10\n",
    " - 170s - loss: 0.0295 - acc: 0.9892\n",
    "Epoch 9/10\n",
    " - 168s - loss: 0.0254 - acc: 0.9904\n",
    "Epoch 10/10\n",
    " - 172s - loss: 0.0239 - acc: 0.9919\n",
    "score: 0.07\n",
    "acc: 0.98\n",
    "sklearn.cross_validation.StratifiedKFold(labels=[0 1 0 ... 1 0 0], n_folds=10, shuffle=True, random_state=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
